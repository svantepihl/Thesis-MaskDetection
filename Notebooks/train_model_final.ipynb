{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-model-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1-tntx1ITYVooTmYRppxDTercZ1RzUfl3",
      "authorship_tag": "ABX9TyPSd2B8oqj1TnUkZGjfYbaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svantepihl/Thesis-MaskDetection/blob/master/train_model_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gQPyLvxSIW"
      },
      "source": [
        "`NOTE!`  \n",
        "`This notebook needs to be ran in a TPU environment`\n",
        "\n",
        "`Dataset loads from a private GCP bucket but can be downloaded from here :` \n",
        "\n",
        "[LINK](https://drive.google.com/drive/folders/18UJsRrjrW4lIlKbYhNQbQcrLFoApxymp?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh7isXMHGKue"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOUjuixaGJ4B"
      },
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpIMf64stMVi"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Rc2y6ftOeT"
      },
      "source": [
        "!pip install -U tensorboard_plugin_profile # Tensorboard plugin for profiling performance "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyHIHIhxyTWl"
      },
      "source": [
        "# Auth for Google Cloud Plattform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34J2Psf8xnsY"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "   from google.colab import auth\n",
        "   auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_hJz_Gyo5A"
      },
      "source": [
        "# Tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJximxwEw7fZ"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf, tensorflow.keras.backend as K\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3lI4FlOy8J7"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wDAJCwgzBDL"
      },
      "source": [
        "# TPU config  \n",
        "Reference: [TPUs in Google Colab](https://colab.research.google.com/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Mzj-AszAiq"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YRg3xqW5fUc"
      },
      "source": [
        "# Constants\n",
        "\n",
        "All constants are captitalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HczCMcfx5hQt"
      },
      "source": [
        "#@title New run?\n",
        "\n",
        "SEED = 1337\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_CHANNELS = 3\n",
        "IMAGE_SIZE = [IMAGE_WIDTH, IMAGE_HEIGHT]\n",
        "IMAGE_SHAPE = [*IMAGE_SIZE, IMAGE_CHANNELS]\n",
        "\n",
        "EPOCHS = 400\n",
        "\n",
        "# TRANINGS SETTINGS\n",
        "if tpu != None:\n",
        "  BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  VALIDATION_BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  TEST_BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  AUG_BATCH = BATCH_SIZE\n",
        "else:\n",
        "  BATCH_SIZE = 64\n",
        "  VALIDATION_BATCH_SIZE = 64\n",
        "  TEST_BATCH_SIZE = 64\n",
        "  AUG_BATCH = BATCH_SIZE\n",
        "\n",
        "GCS_DATASET_PATTERN = 'gs://facemask-detection-thesis-32-tfrecords-jpeg-224x224/*.tfrec' # GCS bucket where dataset is stored\n",
        "\n",
        "GCS_LOG_BUCKET = 'gs://facemask-detection-thesis-training-logs/' # To store training logs for tensorboard\n",
        "\n",
        "NEW_RUN = True #@param {type:\"boolean\"}\n",
        "if NEW_RUN:\n",
        "  now = datetime.now()\n",
        "  dt_string = now.strftime(\"%Y-%m-%d_%H\")\n",
        "\n",
        "  RUN_FOLDER = '/content/drive/MyDrive/MaskedFace/Final/RUN-' + dt_string +'/'\n",
        "  os.makedirs(RUN_FOLDER)\n",
        "  print(\"Created folder: \"+ RUN_FOLDER)\n",
        "\n",
        "  MODEL_FOLDER = RUN_FOLDER + 'Models/'\n",
        "  os.makedirs(MODEL_FOLDER)\n",
        "  print(\"Created folder: \"+ MODEL_FOLDER)\n",
        "\n",
        "CLASSES = ['MaskCorrect', 'MaskOnChin', 'MaskOnlyOnMouth', 'NoMask']\n",
        "\n",
        "TRAIN_AND_VALIDATION_SPLIT = 0.20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k4RzBEt2sWe"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIUrLDnKjOLL"
      },
      "source": [
        "def write_vars_to_file(f, **kwargs):\n",
        "    for name, val in kwargs.items():\n",
        "      f.write(\"%s = %s\\n\" %(name, val))\n",
        "\n",
        "def get_dataset_labels(dataset):\n",
        "  _, labels = tuple(zip(*dataset.unbatch()))\n",
        "  labels = np.array(labels)\n",
        "  return labels\n",
        "\n",
        "def dataset_to_numpy_util(dataset,N):\n",
        "  dataset = dataset.unbatch().shuffle(N).batch(N)\n",
        "  for images, labels in dataset:\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    break;  \n",
        "  return numpy_images, numpy_labels\n",
        "\n",
        "def whole_dataset_to_numpy_util(dataset):\n",
        "  images, labels = tuple(zip(*dataset.unbatch()))\n",
        "  images = np.array(images)\n",
        "  labels = np.array(labels)\n",
        "  return images,labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  label = np.argmax(label, axis=-1)  # one-hot to class number\n",
        "  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_grayscale_image(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    arr = np.asarray(image)\n",
        "    arr = arr[:,:,0]\n",
        "    plt.imshow(arr, cmap='gray', vmin=0, vmax=255) # Grayscale \n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "\n",
        "def display_one_image(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap='gray', vmin=0, vmax=255) # color\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset, grayscale = False):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
        "    if grayscale:\n",
        "      subplot = display_one_grayscale_image(image, title, subplot)\n",
        "    else:\n",
        "      subplot = display_one_image(image, title, subplot)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout() \n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "    subplot = display_one_image(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout() \n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  #ax.set_ylim(0.28,1.05)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypXh238E1QSO"
      },
      "source": [
        "# Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRton5Q185qI"
      },
      "source": [
        "## Rotate, shear, shift, zoom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5q2jSSz9Jds"
      },
      "source": [
        "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    # returns 3x3 transformmatrix which transforms indicies\n",
        "        \n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    shear = math.pi * shear / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
        "    \n",
        "    # ZOOM MATRIX\n",
        "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    # SHIFT MATRIX\n",
        "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUIdKIzp9V-B"
      },
      "source": [
        "def rotate_shear_shift_zoom(image,label):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 14. * tf.random.normal([1],dtype='float32')\n",
        "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
        "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    h_shift = 12. * tf.random.normal([1],dtype='float32') \n",
        "    w_shift = 12. * tf.random.normal([1],dtype='float32') \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    \n",
        "\n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "    \n",
        "    image = tf.reshape(d,[DIM,DIM,3])\n",
        "    return image,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73m_OJlcDCtM"
      },
      "source": [
        "## Image augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cf8vsBDDEi4"
      },
      "source": [
        "def img_augment(image, one_hot_class):\n",
        "    image = tf.image.random_hue(image, 0.05,seed=SEED) \n",
        "    image = tf.image.random_saturation(image, 0.6, 1.5,seed=SEED)\n",
        "    image = tf.image.random_contrast(image, 0.7, 1.3,seed=SEED)\n",
        "    image = tf.image.random_flip_left_right(image,seed=SEED)\n",
        "    image = tf.image.random_brightness(image,0.3)\n",
        "    return image, one_hot_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RL3cic6I22"
      },
      "source": [
        "# Dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI_f_4wT-IO8"
      },
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    # image = (tf.cast(image, tf.float32) / 127.5) - 1  # convertion done in model\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxiHbyNz660"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "  features = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  image = decode_image(example['image'])\n",
        "  one_hot_class = tf.reshape(tf.sparse.to_dense(example['one_hot_class']), [4])\n",
        "  label = tf.cast(example['class'], tf.int32)\n",
        "  return image, one_hot_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR93m0fUlCFh"
      },
      "source": [
        "def load_dataset(filenames, ordered = False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TKQILSvtG_X"
      },
      "source": [
        "def get_training_dataset(dataset):\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.map(img_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(rotate_shear_shift_zoom, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75N-CtqA0hXE"
      },
      "source": [
        "def get_validation_dataset(dataset):\n",
        "    dataset = dataset.batch(VALIDATION_BATCH_SIZE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxYJsj5emmx"
      },
      "source": [
        "# Load data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uXCqmVzemMM"
      },
      "source": [
        "%%time\n",
        "filenames = tf.io.gfile.glob(GCS_DATASET_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifVJQxsW05OP"
      },
      "source": [
        "# Split into train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeA_C26IeToW"
      },
      "source": [
        "TRAIN_FILENAMES, VALIDATION_FILENAMES = sklearn.model_selection.train_test_split(filenames, test_size=TRAIN_AND_VALIDATION_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRCkQKwXebFY"
      },
      "source": [
        "# Summarise dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmZ8G2DQSVPS"
      },
      "source": [
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. 00-2000.tfrec = 2000 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_qNdgT0MZru"
      },
      "source": [
        "NUM_TRAINING_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
        "# use validation data for training\n",
        "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "TRAIN_STEPS = count_data_items(TRAIN_FILENAMES) // BATCH_SIZE\n",
        "print(\"TOTAL IMAGES: \", int(count_data_items(filenames)))\n",
        "print(\"TRAINING IMAGES: \", int(NUM_TRAINING_IMAGES), \", STEPS PER EPOCH: \", int(STEPS_PER_EPOCH))\n",
        "print(\"VALIDATION IMAGES \",int(NUM_VALIDATION_IMAGES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFPGXF9dXf6l"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIim7dQRYotL"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG445M9pXh6c"
      },
      "source": [
        "# Learning rate settings\n",
        "start_lr = 0.000000000000001 \n",
        "min_lr = 0.000000000000001 \n",
        "if tpu != None:\n",
        "  max_lr = 0.00005 * strategy.num_replicas_in_sync\n",
        "else: \n",
        "  max_lr = 0.00005 * 8\n",
        "rampup_epochs = 200\n",
        "sustain_epochs = 50\n",
        "exp_decay = .9\n",
        "\n",
        "# Learning rate function\n",
        "def learning_rate_fn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "\n",
        "\n",
        "# Plot learning rate\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [learning_rate_fn(x) for x in rng]\n",
        "plt.plot(rng, [learning_rate_fn(x) for x in rng])\n",
        "print(y[0], y[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zHNOAmCzrvZ"
      },
      "source": [
        "### Save learning rate settings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iea1A6TBzgVA"
      },
      "source": [
        "lr_file = open(RUN_FOLDER + 'LR.txt',mode='a+')\n",
        "write_vars_to_file(lr_file,\n",
        "                   start_lr=start_lr,\n",
        "                   min_lr=min_lr,\n",
        "                   max_lr=max_lr,\n",
        "                   rampup_epochs=rampup_epochs,\n",
        "                   sustain_epochs=sustain_epochs,\n",
        "                   exp_decay=exp_decay)\n",
        "lr_file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKSSVn8wUqOd"
      },
      "source": [
        "def get_learning_rate():\n",
        "  return tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate_fn(epoch), verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9dmdY8ZIqD"
      },
      "source": [
        "## Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLFz7INbYtht"
      },
      "source": [
        "def get_earlystopping_callback(epoch_patience = 60):\n",
        "  return tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=epoch_patience,\n",
        "    verbose=True\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRQdH0mma4K7"
      },
      "source": [
        "## Model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAJK6Fnza99m"
      },
      "source": [
        "def get_checkpoint_callback(model_name):\n",
        "  checkpoint_path = MODEL_FOLDER + model_name\n",
        "  return tf.keras.callbacks.ModelCheckpoint(checkpoint_path,monitor='val_loss',verbose=1,save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CM61frCtGGp"
      },
      "source": [
        "## Tensorboard callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFffAlStFUT"
      },
      "source": [
        "def get_tensorboard_callback():\n",
        "  logs = GCS_LOG_BUCKET + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir = logs, \n",
        "                                        histogram_freq = 1,\n",
        "                                        profile_batch=(20,50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwP2UHw098ce"
      },
      "source": [
        "# Load and start Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OefbduU9-UJ"
      },
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bw1Hxn3-FcW"
      },
      "source": [
        "# Get TPU profiling service address. This address will be needed for capturing\n",
        "# profile information with TensorBoard in the following steps.\n",
        "service_addr = tpu.get_master().replace(':8470', ':8466')\n",
        "tpu_worker = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466') \n",
        "print(tf.profiler.experimental.client.monitor(tpu_worker,1))\n",
        "print(tpu_worker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkaXDDGT-LIm"
      },
      "source": [
        "# Launch TensorBoard.\n",
        "%tensorboard --logdir=gs://facemask-detection-thesis-training-logs/  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As59usf2R-PJ"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IvjvmKmR9Mi"
      },
      "source": [
        "def create_model():\n",
        "  # Base model\n",
        "  base_model = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    minimalistic=True, \n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        "  )\n",
        "\n",
        "  input = tf.keras.Input(shape=IMAGE_SHAPE)\n",
        "  x = tf.keras.applications.mobilenet_v3.preprocess_input(input)\n",
        "  x = base_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(input, outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      loss = 'categorical_crossentropy',\n",
        "      metrics=['categorical_accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0HpgpRnDTHo"
      },
      "source": [
        "def create_tpu_model():\n",
        "  with strategy.scope():\n",
        "    return create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMo5wJoBCAnn"
      },
      "source": [
        "## Save model settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KxLjFygCDUM"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()\n",
        "json_model = model.to_json()\n",
        "with open(RUN_FOLDER+'model.json', 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "del model, json_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTA7-B-iUwFE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDwMRTPCU-kM"
      },
      "source": [
        "def train_model():\n",
        "  #early_stopping = get_earlystopping_callback()\n",
        "  tensorboard = get_tensorboard_callback()\n",
        "  learning_rate = get_learning_rate()\n",
        "\n",
        "  train_dataset = load_dataset(TRAIN_FILENAMES)\n",
        "  val_dataset = load_dataset(VALIDATION_FILENAMES)\n",
        "\n",
        "  checkpoint_name = f'model_checkpoint' + '.h5'\n",
        "  model_checkpoint = get_checkpoint_callback(checkpoint_name)\n",
        "\n",
        "  model = create_tpu_model()\n",
        "\n",
        "  history = model.fit(\n",
        "            get_training_dataset(train_dataset), \n",
        "            validation_data=get_validation_dataset(val_dataset),\n",
        "            steps_per_epoch=STEPS_PER_EPOCH, \n",
        "            epochs=EPOCHS, \n",
        "            callbacks=[tensorboard,learning_rate,model_checkpoint])\n",
        "  \n",
        "  print('Load best weights for model prediction')\n",
        "  model.load_weights(MODEL_FOLDER + checkpoint_name)\n",
        "\n",
        "  return model,history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1CfMZ4WMaO"
      },
      "source": [
        "final_model, history  = train_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGXEavylYOpP"
      },
      "source": [
        "saved_model_path = MODEL_FOLDER + 'final_model.h5'\n",
        "\n",
        "final_model.save(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXisN1hvYtWI"
      },
      "source": [
        "def display_training_curves(training, validation, title, subplot):\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['training', 'validation'])\n",
        "\n",
        "plt.subplots(figsize=(10,10))\n",
        "plt.tight_layout()\n",
        "display_training_curves(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'categorical_accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRwFefljz6P4"
      },
      "source": [
        "# Must copy the model from TPU to CPU to be able to compose them.\n",
        "restored_model = create_model();\n",
        "restored_model.set_weights(final_model.get_weights()) # this copies the weights from TPU\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEus6FKQGX0F"
      },
      "source": [
        "# Convert to TFLite Model (Not quantized)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oa4n31pimps"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(restored_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_path = MODEL_FOLDER + \"final_model.tflite\"\n",
        "with open(tflite_path, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj2mRhMoHjq_"
      },
      "source": [
        "# Convert to TFLite model (quantaized)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H53mVRI9tJWY"
      },
      "source": [
        "quant_dataset = load_dataset(TRAIN_FILENAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lh4IefyHjBX"
      },
      "source": [
        "def representative_data_gen():\n",
        "  for images,_ in quant_dataset.batch(1).take(200):\n",
        "    images = images.numpy()\n",
        "    images = images.astype(np.float32)\n",
        "    images = tf.keras.applications.mobilenet_v3.preprocess_input(images)\n",
        "    yield [images]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(restored_model)\n",
        "converter.experimental_new_converter = True\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "tflite_quant_path = MODEL_FOLDER + \"final_model_quant.tflite\"\n",
        "with open(tflite_quant_path, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrBKtiJ6sjfv"
      },
      "source": [
        "# Compare quantized vs non-quantized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1GI7Svmx6tV"
      },
      "source": [
        "Get dataset to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXfl05E-x4Y_"
      },
      "source": [
        "validation_dataset = get_validation_dataset(load_dataset(VALIDATION_FILENAMES))\n",
        "\n",
        "batch_images, batch_labels = dataset_to_numpy_util(validation_dataset,5000)\n",
        "#batch_images = tf.keras.applications.mobilenet_v3.preprocess_input(batch_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGxJnJv1_h_2"
      },
      "source": [
        "logits = restored_model(batch_images)\n",
        "prediction = np.argmax(logits, axis=1)\n",
        "truth = np.argmax(batch_labels, axis=1)\n",
        "\n",
        "keras_accuracy = tf.keras.metrics.Accuracy()\n",
        "keras_accuracy(prediction, truth)\n",
        "\n",
        "print(\"Raw model accuracy: {:.3%}\".format(keras_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQcQUfsF-v_t"
      },
      "source": [
        "def set_input_tensor(interpreter, input):\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  tensor_index = input_details['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  # Inputs for the TFLite model must be uint8, so we quantize our input data.\n",
        "  # NOTE: This step is necessary only because we're receiving input data from\n",
        "  # ImageDataGenerator, which rescaled all image data to float [0,1]. When using\n",
        "  # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:\n",
        "  #   input_tensor[:, :] = input\n",
        "  #scale, zero_point = input_details['quantization']\n",
        "  #input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
        "  input_tensor[:, :] = input\n",
        "\n",
        "def classify_image(interpreter, input):\n",
        "  set_input_tensor(interpreter, input)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  output = interpreter.get_tensor(output_details['index'])\n",
        "  # Outputs from the TFLite model are uint8, so we dequantize the results:\n",
        "  scale, zero_point = output_details['quantization']\n",
        "  output = scale * (output - zero_point)\n",
        "  top_1 = np.argmax(output)\n",
        "  return top_1\n",
        "\n",
        "interpreter = tf.lite.Interpreter(tflite_quant_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Collect all inference predictions in a list\n",
        "batch_prediction = []\n",
        "batch_truth = np.argmax(batch_labels, axis=1)\n",
        "\n",
        "for i in range(len(batch_images)):\n",
        "  prediction = classify_image(interpreter, batch_images[i])\n",
        "  batch_prediction.append(prediction)\n",
        "\n",
        "# Compare all predictions to the ground truth\n",
        "tflite_accuracy = tf.keras.metrics.Accuracy()\n",
        "tflite_accuracy(batch_prediction, batch_truth)\n",
        "print(\"Quant TF Lite accuracy: {:.3%}\".format(tflite_accuracy.result()))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-model-k-fold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "14_Rpr9Fg-YvIoBwh3-WO5wWMBEp0Y9ql",
      "authorship_tag": "ABX9TyOzoZKdINisVbft+/vz7h6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svantepihl/Thesis-MaskDetection/blob/master/train_model_k_fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gQPyLvxSIW"
      },
      "source": [
        "`NOTE!`  \n",
        "`This notebook needs to be ran in a TPU environment`\n",
        "\n",
        "`Dataset loads from a private GCP bucket but can be downloaded from here :` \n",
        "\n",
        "[LINK](https://drive.google.com/drive/folders/18UJsRrjrW4lIlKbYhNQbQcrLFoApxymp?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh7isXMHGKue"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOUjuixaGJ4B"
      },
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpIMf64stMVi"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Rc2y6ftOeT"
      },
      "source": [
        "!pip install -U tensorboard_plugin_profile # Tensorboard plugin for profiling performance "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyHIHIhxyTWl"
      },
      "source": [
        "# Auth for Google Cloud Plattform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34J2Psf8xnsY"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "   from google.colab import auth\n",
        "   auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_hJz_Gyo5A"
      },
      "source": [
        "# Tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJximxwEw7fZ"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf, tensorflow.keras.backend as K\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3lI4FlOy8J7"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wDAJCwgzBDL"
      },
      "source": [
        "# TPU config  \n",
        "Reference: [TPUs in Google Colab](https://colab.research.google.com/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Mzj-AszAiq"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YRg3xqW5fUc"
      },
      "source": [
        "# Constants\n",
        "\n",
        "All constants are captitalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HczCMcfx5hQt"
      },
      "source": [
        "#@title Default title text\n",
        "\n",
        "SEED = 1337\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_CHANNELS = 3\n",
        "IMAGE_SIZE = [IMAGE_WIDTH, IMAGE_HEIGHT]\n",
        "IMAGE_SHAPE = [*IMAGE_SIZE, IMAGE_CHANNELS]\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "# TRANINGS SETTINGS\n",
        "if tpu != None:\n",
        "  BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  VALIDATION_BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  TEST_BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "  AUG_BATCH = BATCH_SIZE\n",
        "else:\n",
        "  BATCH_SIZE = 64\n",
        "  VALIDATION_BATCH_SIZE = 64\n",
        "  TEST_BATCH_SIZE = 64\n",
        "  AUG_BATCH = BATCH_SIZE\n",
        "\n",
        "GCS_DATASET_PATTERN = 'gs://facemask-detection-thesis-32-tfrecords-jpeg-224x224/*.tfrec' # GCS bucket where dataset is stored\n",
        "\n",
        "GCS_LOG_BUCKET = 'gs://facemask-detection-thesis-training-logs/' # To store training logs for tensorboard\n",
        "\n",
        "NEW_RUN = False #@param {type:\"boolean\"}\n",
        "if NEW_RUN:\n",
        "  now = datetime.now()\n",
        "  dt_string = now.strftime(\"%Y-%m-%d_%H\")\n",
        "\n",
        "  RUN_FOLDER = '/content/drive/MyDrive/MaskedFace/K-Folds/RUN-' + dt_string +'/'\n",
        "  os.makedirs(RUN_FOLDER)\n",
        "  print(\"Created folder: \"+ RUN_FOLDER)\n",
        "\n",
        "  MODEL_FOLDER = RUN_FOLDER + 'Models/'\n",
        "  os.makedirs(MODEL_FOLDER)\n",
        "  print(\"Created folder: \"+ MODEL_FOLDER)\n",
        "\n",
        "CLASSES = ['MaskCorrect', 'MaskOnChin', 'MaskOnlyOnMouth', 'NoMask']\n",
        "\n",
        "TEST_TRAIN_SPLIT = 0.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hw1Clgfx-r6"
      },
      "source": [
        "print(\"Previous runs:\")\n",
        "print(os.listdir(\"/content/drive/MyDrive/MaskedFace/K-Folds/\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6vDwgY2sTwl"
      },
      "source": [
        "#@title Select previous run\n",
        "PREVIOUS_RUN = \"RUN-2021-05-05_09\" #@param {type:\"string\"}\n",
        "PREVIOUS_RUN_FOLDER = '/content/drive/MyDrive/MaskedFace/K-Folds/' + PREVIOUS_RUN + '/'\n",
        "\n",
        "PREVIOUS_MODELS_FOLDER = PREVIOUS_RUN_FOLDER + '/Models/'\n",
        "\n",
        "print('PREVIOUS_RUN_FOLDER: '+ PREVIOUS_RUN_FOLDER)\n",
        "print('PREVIOUS_MODELS_FOLDER: '+ PREVIOUS_MODELS_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k4RzBEt2sWe"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIUrLDnKjOLL"
      },
      "source": [
        "def write_vars_to_file(f, **kwargs):\n",
        "    for name, val in kwargs.items():\n",
        "      f.write(\"%s = %s\\n\" %(name, val))\n",
        "\n",
        "def get_dataset_labels(dataset):\n",
        "  _, labels = tuple(zip(*dataset.unbatch()))\n",
        "  labels = np.array(labels)\n",
        "  return labels\n",
        "\n",
        "def dataset_to_numpy_util(dataset,N):\n",
        "  dataset = dataset.unbatch().batch(N)\n",
        "  for images, labels in dataset:\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    break;  \n",
        "  return numpy_images, numpy_labels\n",
        "\n",
        "def whole_dataset_to_numpy_util(dataset):\n",
        "  images, labels = tuple(zip(*dataset.unbatch()))\n",
        "  images = np.array(images)\n",
        "  labels = np.array(labels)\n",
        "  return images,labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  label = np.argmax(label, axis=-1)  # one-hot to class number\n",
        "  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_grayscale_image(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    arr = np.asarray(image)\n",
        "    arr = arr[:,:,0]\n",
        "    plt.imshow(arr, cmap='gray', vmin=0, vmax=255) # Grayscale \n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "\n",
        "def display_one_image(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap='gray', vmin=0, vmax=255) # color\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset, grayscale = False):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
        "    if grayscale:\n",
        "      subplot = display_one_grayscale_image(image, title, subplot)\n",
        "    else:\n",
        "      subplot = display_one_image(image, title, subplot)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout() \n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "    subplot = display_one_image(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout() \n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  #ax.set_ylim(0.28,1.05)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypXh238E1QSO"
      },
      "source": [
        "# Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRton5Q185qI"
      },
      "source": [
        "## Rotate, shear, shift, zoom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5q2jSSz9Jds"
      },
      "source": [
        "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    # returns 3x3 transformmatrix which transforms indicies\n",
        "        \n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    shear = math.pi * shear / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
        "    \n",
        "    # ZOOM MATRIX\n",
        "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    # SHIFT MATRIX\n",
        "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUIdKIzp9V-B"
      },
      "source": [
        "def rotate_shear_shift_zoom(image,label):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 14. * tf.random.normal([1],dtype='float32')\n",
        "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
        "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    \n",
        "\n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "    \n",
        "    image = tf.reshape(d,[DIM,DIM,3])\n",
        "    return image,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73m_OJlcDCtM"
      },
      "source": [
        "## Image augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cf8vsBDDEi4"
      },
      "source": [
        "def img_augment(image, one_hot_class):\n",
        "    image = tf.image.random_hue(image, 0.08,seed=SEED) \n",
        "    image = tf.image.random_saturation(image, 0.6, 1.6,seed=SEED)\n",
        "    image = tf.image.random_contrast(image, 0.7, 1.3,seed=SEED)\n",
        "    image = tf.image.random_flip_left_right(image,seed=SEED)\n",
        "    image = tf.image.random_brightness(image,0.5)\n",
        "    return image, one_hot_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RL3cic6I22"
      },
      "source": [
        "# Dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI_f_4wT-IO8"
      },
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    # image = (tf.cast(image, tf.float32) / 127.5) - 1  # convertion done in model\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxiHbyNz660"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "  features = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  image = decode_image(example['image'])\n",
        "  one_hot_class = tf.reshape(tf.sparse.to_dense(example['one_hot_class']), [4])\n",
        "  label = tf.cast(example['class'], tf.int32)\n",
        "  return image, one_hot_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR93m0fUlCFh"
      },
      "source": [
        "def load_dataset(filenames, ordered = False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TKQILSvtG_X"
      },
      "source": [
        "def get_training_dataset(dataset):\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.map(img_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(rotate_shear_shift_zoom, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75N-CtqA0hXE"
      },
      "source": [
        "def get_validation_dataset(dataset):\n",
        "    dataset = dataset.batch(VALIDATION_BATCH_SIZE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBEhBLqmwHPF"
      },
      "source": [
        "def get_test_dataset(ordered=True,prefetch=True,batched=True):\n",
        "    dataset = load_dataset(TEST_FILENAMES, ordered=ordered)\n",
        "    if batched:\n",
        "      dataset = dataset.batch(BATCH_SIZE)\n",
        "    if prefetch: \n",
        "      dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxYJsj5emmx"
      },
      "source": [
        "# Load data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uXCqmVzemMM"
      },
      "source": [
        "%%time\n",
        "filenames = tf.io.gfile.glob(GCS_DATASET_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifVJQxsW05OP"
      },
      "source": [
        "# Split into train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeA_C26IeToW"
      },
      "source": [
        "TRAIN_FILENAMES, TEST_FILENAMES = sklearn.model_selection.train_test_split(filenames, test_size=TEST_TRAIN_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRCkQKwXebFY"
      },
      "source": [
        "# Summarise dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmZ8G2DQSVPS"
      },
      "source": [
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. 00-2000.tfrec = 2000 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_qNdgT0MZru"
      },
      "source": [
        "NUM_TRAINING_IMAGES = count_data_items(TRAIN_FILENAMES) * 0.80\n",
        "NUM_VALIDATION_IMAGES = count_data_items(TRAIN_FILENAMES) * 0.20\n",
        "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "TRAIN_STEPS = count_data_items(TRAIN_FILENAMES) // BATCH_SIZE\n",
        "print(\"TOTAL IMAGES: \", int(count_data_items(filenames)))\n",
        "print(\"TRAINING IMAGES: \", int(NUM_TRAINING_IMAGES), \", STEPS PER EPOCH: \", int(STEPS_PER_EPOCH))\n",
        "print(\"VALIDATION IMAGES \",int(NUM_VALIDATION_IMAGES))\n",
        "print(\"TESTING IMAGES: \", int(NUM_TEST_IMAGES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFPGXF9dXf6l"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIim7dQRYotL"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG445M9pXh6c"
      },
      "source": [
        "# Learning rate settings\n",
        "start_lr = 0.0000001 \n",
        "min_lr = 0.00000001\n",
        "if tpu != None:\n",
        "  max_lr = 0.00005 * strategy.num_replicas_in_sync\n",
        "else: \n",
        "  max_lr = 0.00005\n",
        "rampup_epochs = 50\n",
        "sustain_epochs = 10\n",
        "exp_decay = .6\n",
        "\n",
        "# Learning rate function\n",
        "def learning_rate_fn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "\n",
        "\n",
        "# Plot learning rate\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [learning_rate_fn(x) for x in rng]\n",
        "plt.plot(rng, [learning_rate_fn(x) for x in rng])\n",
        "print(y[0], y[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zHNOAmCzrvZ"
      },
      "source": [
        "### Save learning rate settings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iea1A6TBzgVA"
      },
      "source": [
        "lr_file = open(RUN_FOLDER + 'LR.txt',mode='a+')\n",
        "write_vars_to_file(lr_file,\n",
        "                   start_lr=start_lr,\n",
        "                   min_lr=min_lr,\n",
        "                   max_lr=max_lr,\n",
        "                   rampup_epochs=rampup_epochs,\n",
        "                   sustain_epochs=sustain_epochs,\n",
        "                   exp_decay=exp_decay)\n",
        "lr_file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKSSVn8wUqOd"
      },
      "source": [
        "def get_learning_rate():\n",
        "  return tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate_fn(epoch), verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9dmdY8ZIqD"
      },
      "source": [
        "## Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLFz7INbYtht"
      },
      "source": [
        "def get_earlystopping_callback(epoch_patience = 10):\n",
        "  return tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=epoch_patience,\n",
        "    verbose=True\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRQdH0mma4K7"
      },
      "source": [
        "## Model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAJK6Fnza99m"
      },
      "source": [
        "def get_checkpoint_callback(model_name):\n",
        "  checkpoint_path = MODEL_FOLDER + model_name\n",
        "  return tf.keras.callbacks.ModelCheckpoint(checkpoint_path,monitor='val_loss',save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CM61frCtGGp"
      },
      "source": [
        "## Tensorboard callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFffAlStFUT"
      },
      "source": [
        "def get_tensorboard_callback():\n",
        "  logs = GCS_LOG_BUCKET + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir = logs, \n",
        "                                        histogram_freq = 1,\n",
        "                                        profile_batch=(20,50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwP2UHw098ce"
      },
      "source": [
        "# Load and start Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OefbduU9-UJ"
      },
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bw1Hxn3-FcW"
      },
      "source": [
        "# Get TPU profiling service address. This address will be needed for capturing\n",
        "# profile information with TensorBoard in the following steps.\n",
        "service_addr = tpu.get_master().replace(':8470', ':8466')\n",
        "tpu_worker = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466') \n",
        "print(tf.profiler.experimental.client.monitor(tpu_worker,1))\n",
        "print(tpu_worker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkaXDDGT-LIm"
      },
      "source": [
        "# Launch TensorBoard.\n",
        "%tensorboard --logdir=gs://facemask-detection-thesis-training-logs/  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As59usf2R-PJ"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IvjvmKmR9Mi"
      },
      "source": [
        "def create_model():\n",
        "  # Base model\n",
        "  base_model = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    minimalistic=True, \n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        "  )\n",
        "\n",
        "  input = tf.keras.Input(shape=IMAGE_SHAPE)\n",
        "  x = tf.keras.applications.mobilenet_v3.preprocess_input(input)\n",
        "  x = base_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(input, outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      loss = 'categorical_crossentropy',\n",
        "      metrics=['categorical_accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0HpgpRnDTHo"
      },
      "source": [
        "def create_tpu_model():\n",
        "  with strategy.scope():\n",
        "    return create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMo5wJoBCAnn"
      },
      "source": [
        "## Save model settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KxLjFygCDUM"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()\n",
        "json_model = model.to_json()\n",
        "with open(RUN_FOLDER+'model.json', 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "del model, json_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTA7-B-iUwFE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_qV58skQcEo"
      },
      "source": [
        "def train_cross_validate(folds = 5):\n",
        "    histories = []\n",
        "    models = []\n",
        "    early_stopping = get_earlystopping_callback()\n",
        "    tensorboard = get_tensorboard_callback()\n",
        "    learning_rate = get_learning_rate()\n",
        "    kfold = sklearn.model_selection.KFold(folds, shuffle = True, random_state = SEED)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAIN_FILENAMES)):\n",
        "        print(); print('#'*25)\n",
        "        print('### FOLD',fold+1)\n",
        "        print('#'*25)\n",
        "        train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAIN_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES']), ordered = False)\n",
        "        val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAIN_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES']),  ordered = True)\n",
        "\n",
        "        checkpoint_name = f'model_fold_{fold + 1}' + '.h5'\n",
        "        model_checkpoint = get_checkpoint_callback(checkpoint_name)\n",
        "        \n",
        "        model = create_tpu_model()\n",
        "\n",
        "        history = model.fit(\n",
        "                    get_training_dataset(train_dataset), \n",
        "                    validation_data=get_validation_dataset(val_dataset),\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                    epochs=EPOCHS, \n",
        "                    callbacks=[early_stopping,tensorboard,learning_rate,model_checkpoint])\n",
        "        \n",
        "        print('Load best weights for model prediction')\n",
        "        model.load_weights(MODEL_FOLDER + checkpoint_name)\n",
        "        models.append(model)\n",
        "        histories.append(history)\n",
        "        \n",
        "    return histories, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvCTYfndPRfU"
      },
      "source": [
        "def train_and_test(folds = 5):\n",
        "    print(\"Loading test data\")\n",
        "    test_dataset = get_test_dataset(ordered=True)\n",
        "    test_labels = get_dataset_labels(test_dataset)\n",
        "    print('Start training %i folds'%folds)\n",
        "    histories, models = train_cross_validate(folds = folds)\n",
        "    print('Computing predictions...')\n",
        "    # get the mean probability of the folds models\n",
        "    probabilities = np.average([models[i].predict(test_dataset,verbose=1) for i in range(folds)], axis = 0)\n",
        "    predictions = np.argmax(probabilities, axis=-1)\n",
        "    label_true = np.argmax(test_labels, axis=-1)\n",
        "    print('Printing classification report...')\n",
        "    print(classification_report(label_true, predictions, target_names=CLASSES))\n",
        "    \n",
        "    return histories, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1CfMZ4WMaO"
      },
      "source": [
        "histories, models = train_and_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxpCyWMwC_lk"
      },
      "source": [
        "# Evaluate models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLW9Tdj5vxQq"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tfxo4zvOkvB"
      },
      "source": [
        "print(\"Loading test labels..\")\n",
        "test_dataset = get_test_dataset(ordered=True)\n",
        "test_labels = get_dataset_labels(test_dataset)\n",
        "print(\"Test data loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6g5venUJA59"
      },
      "source": [
        "## Run evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuRTPOFz0uN"
      },
      "source": [
        "\n",
        "evaluations = []\n",
        "predictions = []\n",
        "\n",
        "for fold,model in enumerate(models):\n",
        "  print(\"=================== Test av modell frÃ¥n fold \" + str(fold+1) + \" ===================\\n\")\n",
        "  evaluation = model.evaluate(test_dataset,verbose=0)\n",
        "  evaluations.append(evaluation)\n",
        "  probabilities = model.predict(test_dataset,verbose=0)\n",
        "  prediction = np.argmax(probabilities, axis=-1)\n",
        "  predictions.append(prediction)\n",
        "  label_true = np.argmax(test_labels, axis=-1)\n",
        "  print(classification_report(label_true, prediction, target_names=CLASSES,digits=6))\n",
        "  print('      test loss  ', round(evaluations[0][0],6))\n",
        "  print('\\n')\n",
        "  time.sleep(5)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}